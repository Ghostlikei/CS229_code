{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Spam classification\n",
    "\n",
    "Move the original code from nb.py first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def readMatrix(file):\n",
    "    fd = open(file, 'r')\n",
    "    hdr = fd.readline()\n",
    "    rows, cols = [int(s) for s in fd.readline().strip().split()]\n",
    "    tokens = fd.readline().strip().split()\n",
    "    matrix = np.zeros((rows, cols))\n",
    "    Y = []\n",
    "    for i, line in enumerate(fd):\n",
    "        nums = [int(x) for x in line.strip().split()]\n",
    "        Y.append(nums[0])\n",
    "        kv = np.array(nums[1:])\n",
    "        k = np.cumsum(kv[:-1:2])\n",
    "        v = kv[1::2]\n",
    "        matrix[i, k] = v\n",
    "    return matrix, tokens, np.array(Y)\n",
    "\n",
    "trainMatrix, tokenlist, trainCategory = readMatrix('MATRIX.TRAIN')\n",
    "testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')\n",
    "\n",
    "print(trainCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_probs': array([0.50093284, 0.49906716]), 'token_probs': array([[2.27032946e-04, 9.34841544e-04, 1.60258550e-04, ...,\n",
      "        2.67097584e-05, 5.74259806e-04, 1.86968309e-04],\n",
      "       [2.70691570e-04, 8.21408903e-04, 1.21344497e-04, ...,\n",
      "        3.92036067e-04, 2.89359954e-03, 1.40012881e-04]]), 'token_counts': array([[ 16.,  69.,  11., ...,   1.,  42.,  13.],\n",
      "       [ 28.,  87.,  12., ...,  41., 309.,  14.]])}\n"
     ]
    }
   ],
   "source": [
    "def nb_train(matrix, category):\n",
    "    state = {}\n",
    "    N = matrix.shape[1]\n",
    "    num_classes = len(np.unique(category)) # In this case, num_classes should be 2\n",
    "\n",
    "    # Calculate class probabilities\n",
    "    state['class_probs'] = np.bincount(category) / len(category) # p(y)\n",
    "\n",
    "    # Calculate token probabilities for each class\n",
    "    state['token_probs'] = np.zeros((num_classes, N)) \n",
    "    state['token_counts'] = np.zeros((num_classes, N))\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        docs_in_class = matrix[category == c]\n",
    "        total_tokens_in_class = np.sum(docs_in_class)\n",
    "        state['token_counts'][c] = np.sum(docs_in_class, axis=0)\n",
    "        state['token_probs'][c] = (state['token_counts'][c] + 1) / (total_tokens_in_class + N)\n",
    "\n",
    "    return state\n",
    "\n",
    "state = nb_train(trainMatrix, trainCategory)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0163\n"
     ]
    }
   ],
   "source": [
    "def nb_test(matrix, state):\n",
    "    num_classes, N = state['token_probs'].shape\n",
    "    num_docs = matrix.shape[0]\n",
    "    output = np.zeros(num_docs)\n",
    "\n",
    "    for i in range(num_docs):\n",
    "        doc = matrix[i]\n",
    "        scores = np.zeros(num_classes)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            scores[c] = np.log(state['class_probs'][c])\n",
    "\n",
    "            for j in range(N):\n",
    "                if doc[j] > 0:\n",
    "                    scores[c] += doc[j] * np.log(state['token_probs'][c, j])\n",
    "\n",
    "        output[i] = np.argmax(scores)\n",
    "\n",
    "    return output\n",
    "\n",
    "def evaluate(output, label):\n",
    "    error = np.mean(output != label)\n",
    "    print('Error: %1.4f' % error)\n",
    "\n",
    "output = nb_test(testMatrix, state)\n",
    "\n",
    "evaluate(output, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most indicative tokens for SPAM class:\n",
      "1. httpaddr\n",
      "2. spam\n",
      "3. unsubscrib\n",
      "4. ebai\n",
      "5. valet\n"
     ]
    }
   ],
   "source": [
    "def find_most_indicative_tokens(state, tokenlist, num_tokens=5):\n",
    "    log_ratios = np.log(state['token_probs'][1] / state['token_probs'][0])\n",
    "    sorted_indices = np.argsort(log_ratios)[::-1]  # Sort in descending order\n",
    "\n",
    "    most_indicative_tokens = [tokenlist[i] for i in sorted_indices[:num_tokens]]\n",
    "    return most_indicative_tokens\n",
    "\n",
    "most_indicative_tokens = find_most_indicative_tokens(state, tokenlist)\n",
    "print(\"Most indicative tokens for SPAM class:\")\n",
    "for i, token in enumerate(most_indicative_tokens):\n",
    "    print(f\"{i+1}. {token}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the training size then plot the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 50, Test Error: 0.0387\n",
      "Training size: 100, Test Error: 0.0262\n",
      "Training size: 200, Test Error: 0.0262\n",
      "Training size: 400, Test Error: 0.0187\n",
      "Training size: 800, Test Error: 0.0175\n",
      "Training size: 1400, Test Error: 0.0163\n",
      "Best training size: 1400, Best Test Error: 0.0163\n"
     ]
    }
   ],
   "source": [
    "def size_test():\n",
    "    testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')\n",
    "\n",
    "    training_sizes = [50, 100, 200, 400, 800, 1400]\n",
    "    test_errors = []\n",
    "\n",
    "    for size in training_sizes:\n",
    "        trainMatrix, _, trainCategory = readMatrix(f'MATRIX.TRAIN.{size}')\n",
    "        state = nb_train(trainMatrix, trainCategory)\n",
    "        output = nb_test(testMatrix, state)\n",
    "        error = np.mean(output != testCategory)\n",
    "        test_errors.append(error)\n",
    "\n",
    "    for size, error in zip(training_sizes, test_errors):\n",
    "        print(f\"Training size: {size}, Test Error: {error:.4f}\")\n",
    "\n",
    "    best_size = training_sizes[np.argmin(test_errors)]\n",
    "    print(f\"Best training size: {best_size}, Best Test Error: {min(test_errors):.4f}\")\n",
    "\n",
    "\n",
    "size_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training the same data with SVM with gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 50, Test Error: 0.02625\n",
      "Training size: 100, Test Error: 0.005\n",
      "Training size: 200, Test Error: 0.0025\n",
      "Training size: 400, Test Error: 0.00125\n",
      "Training size: 800, Test Error: 0.0\n",
      "Training size: 1400, Test Error: 0.0\n",
      "Best training size: 800, Best Test Error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "tau = 8.\n",
    "\n",
    "def readMatrix(file):\n",
    "    fd = open(file, 'r')\n",
    "    hdr = fd.readline()\n",
    "    rows, cols = [int(s) for s in fd.readline().strip().split()]\n",
    "    tokens = fd.readline().strip().split()\n",
    "    matrix = np.zeros((rows, cols))\n",
    "    Y = []\n",
    "    for i, line in enumerate(fd):\n",
    "        nums = [int(x) for x in line.strip().split()]\n",
    "        Y.append(nums[0])\n",
    "        kv = np.array(nums[1:])\n",
    "        k = np.cumsum(kv[:-1:2])\n",
    "        v = kv[1::2]\n",
    "        matrix[i, k] = v\n",
    "    category = (np.array(Y) * 2) - 1\n",
    "    return matrix, tokens, category\n",
    "\n",
    "def svm_train(matrix, category):\n",
    "    state = {}\n",
    "    M, N = matrix.shape\n",
    "    #####################\n",
    "    Y = category\n",
    "    matrix = 1. * (matrix > 0)\n",
    "    squared = np.sum(matrix * matrix, axis=1)\n",
    "    gram = matrix.dot(matrix.T)\n",
    "    K = np.exp(-(squared.reshape((1, -1)) + squared.reshape((-1, 1)) - 2 * gram) / (2 * (tau ** 2)) )\n",
    "\n",
    "    alpha = np.zeros(M)\n",
    "    alpha_avg = np.zeros(M)\n",
    "    L = 1. / (64 * M)\n",
    "    outer_loops = 40\n",
    "\n",
    "    alpha_avg\n",
    "    for ii in range(outer_loops * M):\n",
    "        i = int(np.random.rand() * M)\n",
    "        margin = Y[i] * np.dot(K[i, :], alpha)\n",
    "        grad = M * L * K[:, i] * alpha[i]\n",
    "        if (margin < 1):\n",
    "            grad -=  Y[i] * K[:, i]\n",
    "        alpha -=  grad / np.sqrt(ii + 1)\n",
    "        alpha_avg += alpha\n",
    "\n",
    "    alpha_avg /= (ii + 1) * M\n",
    "\n",
    "    state['alpha'] = alpha\n",
    "    state['alpha_avg'] = alpha_avg\n",
    "    state['Xtrain'] = matrix\n",
    "    state['Sqtrain'] = squared\n",
    "    ####################\n",
    "    return state\n",
    "\n",
    "def svm_test(matrix, state):\n",
    "    M, N = matrix.shape\n",
    "    output = np.zeros(M)\n",
    "    ###################\n",
    "    Xtrain = state['Xtrain']\n",
    "    Sqtrain = state['Sqtrain']\n",
    "    matrix = 1. * (matrix > 0)\n",
    "    squared = np.sum(matrix * matrix, axis=1)\n",
    "    gram = matrix.dot(Xtrain.T)\n",
    "    K = np.exp(-(squared.reshape((-1, 1)) + Sqtrain.reshape((1, -1)) - 2 * gram) / (2 * (tau ** 2)))\n",
    "    alpha_avg = state['alpha_avg']\n",
    "    preds = K.dot(alpha_avg)\n",
    "    output = np.sign(preds)\n",
    "    ###################\n",
    "    return output\n",
    "\n",
    "def evaluate(output, label):\n",
    "    error = (output != label).sum() * 1. / len(output)\n",
    "    return error\n",
    "\n",
    "def size_test_svm():\n",
    "    testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')\n",
    "\n",
    "    training_sizes = [50, 100, 200, 400, 800, 1400]\n",
    "    test_errors = []\n",
    "\n",
    "    for size in training_sizes:\n",
    "        trainMatrix, _, trainCategory = readMatrix(f'MATRIX.TRAIN.{size}')\n",
    "        state = svm_train(trainMatrix, trainCategory)\n",
    "        output = svm_test(testMatrix, state)\n",
    "        error = evaluate(output, testCategory)\n",
    "        test_errors.append(error)\n",
    "\n",
    "    for i in range(len(training_sizes)):\n",
    "        print(f\"Training size: {training_sizes[i]}, Test Error: {test_errors[i]}\")\n",
    "\n",
    "    best_size = training_sizes[np.argmin(test_errors)]\n",
    "    print(f\"Best training size: {best_size}, Best Test Error: {min(test_errors):.4f}\")\n",
    "\n",
    "\n",
    "size_test_svm()\n",
    "\n",
    "# trainMatrix, tokenlist, trainCategory = readMatrix('MATRIX.TRAIN.400')\n",
    "# testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')\n",
    "\n",
    "# state = svm_train(trainMatrix, trainCategory)\n",
    "# output = svm_test(testMatrix, state)\n",
    "\n",
    "# evaluate(output, testCategory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroDase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
